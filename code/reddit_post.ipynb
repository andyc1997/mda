{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='blue'> Notebook description</font>**\n",
    "\n",
    "<font color='blue'>We use package *praw* to extract comments from Reddit. Please <ins>install</ins> and <ins>login with your reddit account</ins>, and <ins>create user application</ins> before running the script.</font>\n",
    "\n",
    "- <font color='blue'>username: *watashimikami*</font>\n",
    "- <font color='blue'>password: *mda2021thailand*</font>\n",
    "- <font color='blue'>For client_id and client_secret, visit [https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps), and click script on create application section.</font>\n",
    "- <font color='blue'>user_agent: any string</font>\n",
    "\n",
    "**<font color='red'> Please do NOT share the reddit account!</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a reddit scraper object\n",
    "reddit = praw.Reddit(client_id = 'h9WMn8fyPG0Dxg', \n",
    "                     client_secret = 'jM3QrDiSTNOnk1zA_dGhe6F_B3D4HA',\n",
    "                     user_agent = 'Comment Extraction (by u/watashimikami)', \n",
    "                     username = 'watashimikami',\n",
    "                     password = 'mda2021thailand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='green'> Reddit post</font>**\n",
    "\n",
    "- <font color='green'> The post in our study: </font>\n",
    "['Hell is coming': Western Europe braces for its hottest weather since a 2003 heat wave killed 15,000 people in France.](https://www.reddit.com/r/worldnews/comments/c5e7zr/hell_is_coming_western_europe_braces_for_its/)\n",
    "- <font color='green'> The post is about 1 year ago, and the scraping time may take 8 to 10 minutes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reddit post that we are trying to scrape\n",
    "url = 'https://www.reddit.com/r/worldnews/comments/c5e7zr/hell_is_coming_western_europe_braces_for_its/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape from reddit\n",
    "submission = reddit.submission(url = url)\n",
    "\n",
    "# Get comments from all levels\n",
    "submission.comments.replace_more(limit = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='green'> Data description</font>**\n",
    "- <font color='green'> *id*: An unique identifier of the comment.</font>\n",
    "- <font color='green'> *comment*: Text of the comment.</font>\n",
    "- <font color='green'> *upvotes*: The number of upvotes of the comment.</font>\n",
    "- <font color='green'> *downvotes*: The number of downvotes of the comment.</font>\n",
    "- <font color='green'> *time*: When is the time that the comment is made.</font>\n",
    "- <font color='green'> *parent_id*: An unique identifier of the parenet of the comment.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store comments into a dataframe if it is not a deleted/removed comment\n",
    "df_reddit = pd.DataFrame()\n",
    "\n",
    "for comment in submission.comments.list():\n",
    "    if comment.body not in ['[deleted]', '[removed]']:\n",
    "        df_reddit = df_reddit.append({'id': comment.id, \n",
    "                                   'comment': comment.body.replace(';', ','), \n",
    "                                   'upvotes': comment.ups, \n",
    "                                   'downvotes': comment.downs, \n",
    "                                   'time': str(pd.to_datetime(datetime.fromtimestamp(comment.created))), \n",
    "                                   'parent_id': comment.parent_id}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scraped comment with unique delimiter\n",
    "df_reddit.to_csv('../data/reddit_heatwaves_comment.csv', sep = ';', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
